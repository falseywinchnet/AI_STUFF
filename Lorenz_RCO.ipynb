{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgUFA7l8snoTkR/d39sR2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falseywinchnet/AI_STUFF/blob/main/Lorenz_RCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class RCO_Batching(Optimizer):\n",
        "    \"\"\"\n",
        "    Runge-Kutta-Chebyshev Optimizer (RCO) with batching support, featuring self-decaying learning rate\n",
        "    batching necessitates lowering the learn rate- this may not beat adam\n",
        "    it also required clamping, depending on the problem.. YRMV\n",
        "    \"\"\"\n",
        "    def __init__(self, model, max_batch_size=None):\n",
        "        self.model = model\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.scaling_factor = 0.9\n",
        "\n",
        "        # Pre-compute constants\n",
        "        pi = torch.tensor(math.pi)\n",
        "        self.w1 = self.w2 = 4/3\n",
        "        self.w3 = self.w4 =  4/3\n",
        "\n",
        "        # Trig constants\n",
        "        self.cos_3pi8 = torch.cos(3*pi/8)\n",
        "        self.cos_pi8 = torch.cos(pi/8)\n",
        "\n",
        "    def compute_loss(self, x, y):\n",
        "        y_pred = self.model(x)\n",
        "        return torch.mean((y_pred - y)**2)\n",
        "\n",
        "    def _split_batch(self, x, y):\n",
        "        \"\"\"Split large batches into smaller ones if needed\"\"\"\n",
        "        if self.max_batch_size is None or x.size(0) <= self.max_batch_size:\n",
        "            return [(x, y)]\n",
        "\n",
        "        num_splits = (x.size(0) + self.max_batch_size - 1) // self.max_batch_size\n",
        "        x_splits = torch.split(x, self.max_batch_size)\n",
        "        y_splits = torch.split(y, self.max_batch_size)\n",
        "        return list(zip(x_splits, y_splits))\n",
        "\n",
        "    def step(self, x, y):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step using combined RK4-Chebyshev method.\n",
        "        \"\"\"\n",
        "        # Initial k1\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k1 = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # Store original params\n",
        "        orig_params = [p.data.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # RK4 steps\n",
        "        for p, k in zip(self.model.parameters(), k1):\n",
        "            p.data -= k * ((6/  x.size(0)))\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k2_rk4 = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # Reset and move to k3 position\n",
        "        for p, orig in zip(self.model.parameters(), orig_params):\n",
        "            p.data.copy_(orig)\n",
        "        for p, k in zip(self.model.parameters(), k2_rk4):\n",
        "            p.data -= k * ((4/  x.size(0)))\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k3_rk4 = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # Reset and move to k4\n",
        "        for p, orig in zip(self.model.parameters(), orig_params):\n",
        "            p.data.copy_(orig)\n",
        "        for p, k in zip(self.model.parameters(), k3_rk4):\n",
        "            p.data -= k * ((12/  x.size(0)))\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k4_rk4 = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # Chebyshev steps\n",
        "        # Reset for Chebyshev\n",
        "        for p, orig in zip(self.model.parameters(), orig_params):\n",
        "            p.data.copy_(orig)\n",
        "\n",
        "        # Chebyshev nodes with learning rate\n",
        "        c1 = ((12/  x.size(0))) * (1 + self.cos_3pi8)/2\n",
        "        c2 = ((12/  x.size(0))) * (1 + self.cos_pi8)/2\n",
        "        c3 = ((12/  x.size(0))) * (1 - self.cos_pi8)/2\n",
        "\n",
        "        # k2 Cheb\n",
        "        for p, k in zip(self.model.parameters(), k1):\n",
        "            p.data -= k * c1\n",
        "        loss = self.compute_loss(x, y)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k2_cheb = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # k3 Cheb\n",
        "        for p, orig in zip(self.model.parameters(), orig_params):\n",
        "            p.data.copy_(orig)\n",
        "        for p, k in zip(self.model.parameters(), k2_cheb):\n",
        "            p.data -= k * c2\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k3_cheb = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # k4 Cheb\n",
        "        for p, orig in zip(self.model.parameters(), orig_params):\n",
        "            p.data.copy_(orig)\n",
        "        for p, k in zip(self.model.parameters(), k3_cheb):\n",
        "            p.data -= k * c3\n",
        "        loss = self.compute_loss(x, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Fixed line\n",
        "\n",
        "        k4_cheb = [p.grad.clone() for p in self.model.parameters()]\n",
        "\n",
        "        # Combine RK4 result\n",
        "        rk4_update = []\n",
        "        for k1_p, k2_p, k3_p, k4_p in zip(k1, k2_rk4, k3_rk4, k4_rk4):\n",
        "            update = (k1_p + 2*k2_p + 3*k3_p + k4_p)/((6))\n",
        "            rk4_update.append(update)\n",
        "\n",
        "        # Combine Cheb result\n",
        "        cheb_update = []\n",
        "        for k1_p, k2_p, k3_p, k4_p in zip(k1, k2_cheb, k3_cheb, k4_cheb):\n",
        "            update = (self.w1*k1_p + self.w2*k2_p + self.w3*k3_p + self.w4*k4_p)/(16)\n",
        "            cheb_update.append(update)\n",
        "\n",
        "        # Average the two methods and apply final update\n",
        "        for i, (p, rk4_u, cheb_u) in enumerate(zip(self.model.parameters(), rk4_update, cheb_update)):\n",
        "            update = (rk4_u + cheb_u)/2\n",
        "            p.data -=( update * ((12/  x.size(0))) ) *self.scaling_factor\n",
        "            p.grad.zero_()\n",
        "\n",
        "        final_loss = self.compute_loss(x, y)\n",
        "        return final_loss.item()"
      ],
      "metadata": {
        "id": "iA_QfyZgh0b8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xZFsp0t8hud8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.integrate import odeint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, let's create our ground truth data generator\n",
        "class LorenzSystem:\n",
        "    def __init__(self, sigma=10.0, rho=28.0, beta=8/3):\n",
        "        self.sigma = sigma\n",
        "        self.rho = rho\n",
        "        self.beta = beta\n",
        "\n",
        "    def derivatives(self, state, t):\n",
        "        x, y, z = state\n",
        "        dx = self.sigma * (y - x)\n",
        "        dy = x * (self.rho - z) - y\n",
        "        dz = x * y - self.beta * z\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def generate_trajectory(self, initial_state, t_span, n_points=1000):\n",
        "        t = np.linspace(t_span[0], t_span[1], n_points)\n",
        "        trajectory = odeint(self.derivatives, initial_state, t)\n",
        "        return t, trajectory\n",
        "\n",
        "# Neural network to predict Lorenz dynamics\n",
        "class LorenzPredictor(nn.Module):\n",
        "    def __init__(self, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Generate training data\n",
        "def generate_training_data(n_trajectories=100, n_points=1000, t_span=(0, 10)):\n",
        "    lorenz = LorenzSystem()\n",
        "    all_trajectories = []\n",
        "\n",
        "    for _ in range(n_trajectories):\n",
        "        # Random initial conditions\n",
        "        initial_state = np.random.randn(3) * 0.1\n",
        "        _, trajectory = lorenz.generate_trajectory(initial_state, t_span, n_points)\n",
        "        all_trajectories.append(trajectory)\n",
        "\n",
        "    return np.vstack(all_trajectories)\n",
        "\n",
        "# Training setup\n",
        "def prepare_training_data(trajectories, sequence_length=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(trajectories) - sequence_length):\n",
        "        X.append(trajectories[i])\n",
        "        y.append(trajectories[i + 1])  # Predict next state\n",
        "    return torch.FloatTensor(X), torch.FloatTensor(y)\n",
        "\n",
        "# Training function\n",
        "# Modify training function to add gradient clipping\n",
        "def train_model(model, optimizer, train_X, train_y, n_epochs=1, batch_size=128):\n",
        "    criterion = nn.MSELoss()\n",
        "    batch_losses = []\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Shuffle data at start of each epoch\n",
        "        indices = torch.randperm(len(train_X))\n",
        "        train_X = train_X[indices]\n",
        "        train_y = train_y[indices]\n",
        "\n",
        "        for i in range(0, len(train_X), batch_size):\n",
        "            batch_X = train_X[i:i+batch_size].to(device)\n",
        "            batch_y = train_y[i:i+batch_size].to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                pred = model(batch_X)\n",
        "                loss = criterion(pred, batch_y)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "\n",
        "            # For RCO\n",
        "            if isinstance(optimizer, RCO_Batching):\n",
        "                loss = optimizer.step(batch_X, batch_y)\n",
        "            else:  # For other optimizers like Adam\n",
        "                loss = optimizer.step(closure)\n",
        "\n",
        "            batch_losses.append(loss if isinstance(loss, float) else loss.item())\n",
        "\n",
        "            if i % (batch_size * 10) == 0:\n",
        "                avg_loss = sum(batch_losses[-10:]) / min(10, len(batch_losses))\n",
        "                print(f'Epoch {epoch}, Batch {i//batch_size}, Avg Loss: {avg_loss:.6f}')\n",
        "\n",
        "    return batch_losses\n",
        "\n",
        "\n",
        "\n",
        "# Main experiment\n",
        "def run_experiment():\n",
        "    # Generate data\n",
        "    trajectories = generate_training_data()\n",
        "    train_X, train_y = prepare_training_data(trajectories)\n",
        "\n",
        "    # Models and optimizers\n",
        "    model_rco = LorenzPredictor().cuda()\n",
        "    model_adam = LorenzPredictor().cuda()\n",
        "\n",
        "    optimizer_rco = RCO_Batching(model_rco)\n",
        "    optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train both models\n",
        "    losses_rco = train_model(model_rco, optimizer_rco, train_X, train_y)\n",
        "    losses_adam = train_model(model_adam, optimizer_adam, train_X, train_y)\n",
        "\n",
        "    return losses_rco, losses_adam\n",
        "\n",
        "\n",
        "def plot_results(losses_rco, losses_adam):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses_rco, label='RCO')\n",
        "    plt.plot(losses_adam, label='Adam')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Single Epoch Loss Trajectory: RCO vs Adam')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_rco, losses_adam = run_experiment()\n",
        "plot_results(losses_rco, losses_adam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "DFr4Xq7ze8o6",
        "outputId": "94cab92b-94d4-48d7-8331-315521408c5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c19762eaac58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses_rco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_rco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_adam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b51f11da4fe8>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Models and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmodel_rco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLorenzPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mmodel_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLorenzPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
}